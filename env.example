OPENROUTER_API_KEY=

OPENROUTER_DEFAULT_MODEL=
OPENROUTER_DEFAULT_MAX_TOKENS=


OPENROUTER_SINGLE_TOOL_MODEL=
OPENROUTER_SINGLE_TOOL_MAX_TOKENS=

OPENROUTER_JOURNEY_NODE_MODEL=
OPENROUTER_JOURNEY_NODE_MAX_TOKENS=

OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL=
OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS=

OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL=
OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS=


OPENROUTER_EMBEDDING_MODEL=
OPENROUTER_EMBEDDING_API_KEY=
OPENROUTER_EMBEDDING_DIMENSIONS=
OPENROUTER_EMBEDDING_BASE_URL=

MONGODB_URL=
AGENT_CONFIGS_HOST=

# Engine optimization settings

# Maximum number of tool evaluation attempts per tool (default: 2)
# Controls retries when evaluating whether/how to call a tool
# IMPORTANT: Setting to 1 is too aggressive - LLM output quality varies
# Recommended: 2 (balances reliability and cost)
MAX_TOOL_EVALUATION_ATTEMPTS=2

# Maximum number of guideline proposition attempts (default: 2)
# Controls retries for agent initialization evaluations (GuidelineContinuousProposer, etc.)
# These can timeout and consume lots of tokens if retried too many times
# Recommended: 2 (allows one retry for timeouts/errors)
MAX_GUIDELINE_PROPOSITION_ATTEMPTS=2

# Maximum number of engine iterations per request (default: 1-2)
# Controls how many times the engine will retry tool execution after failures
# This is the biggest token saver - keep at 1 for production
# Recommended: 1 (most effective optimization)
MAX_ENGINE_ITERATIONS=1

# Maximum number of history messages to include in LLM context (default: 40)
# Controls how many recent messages are sent to the LLM for each request
# This is crucial for managing token consumption in long conversations
# 
# Benefits:
# - Predictable token consumption for long sessions
# - Significant cost savings (can reduce tokens by 20-50% for long conversations)
# - Faster response times
#
# Recommended values:
# - Short sessions (customer service): 20-30 messages (~10-15 conversation turns)
# - Medium sessions (default): 40 messages (~20 turns)
# - Long sessions: 60-80 messages (~30-40 turns)
# - No limit: Set to 0 or negative number (loads all history)
#
# Note: All related events (STATUS, TOOL_CALL) for these messages are also included
MAX_SESSIONS_CACHED=40

# Maximum number of interaction events to include in LLM context (default: 0 = no limit)
# Controls how many recent interaction events are sent to the LLM for each request
# This includes all event types (MESSAGE, TOOL, CUSTOM, etc.) but excludes STATUS events
# 
# Benefits:
# - Prevents context explosion in long conversations
# - Significant token savings (can reduce tokens by 30-70% for very long sessions)
# - Faster response times
# - More predictable performance
#
# Recommended values:
# - Short sessions: 50 events
# - Medium sessions: 100 events  
# - Long sessions: 150-200 events
# - No limit (default): Set to 0 (loads all events)
#
# Note: Only recent events are kept, older events are truncated
MAX_INTERACTION_EVENTS=0