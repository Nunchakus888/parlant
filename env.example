OPENROUTER_API_KEY=

OPENROUTER_DEFAULT_MODEL=
OPENROUTER_DEFAULT_MAX_TOKENS=


OPENROUTER_SINGLE_TOOL_MODEL=
OPENROUTER_SINGLE_TOOL_MAX_TOKENS=

OPENROUTER_JOURNEY_NODE_MODEL=
OPENROUTER_JOURNEY_NODE_MAX_TOKENS=

OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL=
OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS=

OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL=
OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS=


OPENROUTER_EMBEDDING_MODEL=
OPENROUTER_EMBEDDING_API_KEY=
OPENROUTER_EMBEDDING_DIMENSIONS=
OPENROUTER_EMBEDDING_BASE_URL=

MONGODB_URL=
AGENT_CONFIGS_HOST=

# Engine optimization settings

# Maximum number of tool evaluation attempts per tool (default: 2)
# Controls retries when evaluating whether/how to call a tool
# IMPORTANT: Setting to 1 is too aggressive - LLM output quality varies
# Recommended: 2 (balances reliability and cost)
MAX_TOOL_EVALUATION_ATTEMPTS=2

# Maximum number of guideline proposition attempts (default: 2)
# Controls retries for agent initialization evaluations (GuidelineContinuousProposer, etc.)
# These can timeout and consume lots of tokens if retried too many times
# Recommended: 2 (allows one retry for timeouts/errors)
MAX_GUIDELINE_PROPOSITION_ATTEMPTS=2

# Maximum number of engine iterations per request (default: 1-2)
# Controls how many times the engine will retry tool execution after failures
# This is the biggest token saver - keep at 1 for production
# Recommended: 1 (most effective optimization)
MAX_ENGINE_ITERATIONS=1

# Maximum number of history messages to include in LLM context (default: 40)
# Controls how many recent messages are sent to the LLM for each request
# This is crucial for managing token consumption in long conversations
# 
# Benefits:
# - Predictable token consumption for long sessions
# - Significant cost savings (can reduce tokens by 20-50% for long conversations)
# - Faster response times
#
# Recommended values:
# - Short sessions (customer service): 20-30 messages (~10-15 conversation turns)
# - Medium sessions (default): 40 messages (~20 turns)
# - Long sessions: 60-80 messages (~30-40 turns)
# - No limit: Set to 0 or negative number (loads all history)
#
# Note: All related events (STATUS, TOOL_CALL) for these messages are also included
MAX_SESSIONS_CACHED=40

# Maximum number of interaction events to include in LLM context (default: 0 = no limit)
# Controls how many recent interaction events are sent to the LLM for each request
# This includes all event types (MESSAGE, TOOL, CUSTOM, etc.) but excludes STATUS events
# 
# Benefits:
# - Prevents context explosion in long conversations
# - Significant token savings (can reduce tokens by 30-70% for very long sessions)
# - Faster response times
# - More predictable performance
#
# Recommended values:
# - Short sessions: 50 events
# - Medium sessions: 100 events  
# - Long sessions: 150-200 events
# - No limit (default): Set to 0 (loads all events)
#
# Note: Only recent events are kept, older events are truncated
MAX_INTERACTION_EVENTS=0

# Maximum number of interaction events for tool calling (default: 10)
# Tool calling focuses on recent interactions for parameter extraction and doesn't 
# require full conversation history. This significantly reduces token consumption.
#
# Benefits:
# - Major token savings in tool-heavy workflows (30-70% reduction)
# - Faster tool calling without affecting accuracy
# - Most tool parameters come from recent messages
#
# Recommended values:
# - Simple tools (weather, calculator): 5-10 events
# - Standard tools (default): 10 events (~5 conversation rounds)
# - Complex multi-step tools: 15-20 events
# - No limit: Set to 0 or negative (not recommended)
#
# Note: This only affects tool calling prompts, not message generation
MAX_HISTORY_FOR_TOOL_CALLS=10

# Maximum number of interaction events for message generation (default: 30)
# Message generation needs more context than tool calling but should still be limited
# to prevent unbounded token growth in very long conversations.
#
# Benefits:
# - Prevents exponential token growth in extended conversations
# - Maintains conversation quality while controlling costs
# - Balances context richness with efficiency
#
# Recommended values:
# - Short conversations: 20 events (~10 rounds)
# - Standard conversations (default): 30 events (~15 rounds)
# - Long conversations: 40-60 events (~20-30 rounds)
# - No limit: Set to 0 or negative (allows unlimited history)
#
# Note: This affects all message generation (responses, canned responses, etc.)
MAX_HISTORY_FOR_MESSAGE_GENERATION=30

# Maximum number of interaction events for guideline matching (default: 20)
# Guideline matching evaluates which guidelines apply to the current conversation state.
# Limiting history significantly reduces token consumption (typically 50-70% of total).
#
# Benefits:
# - Major token savings in guideline-heavy workflows (50-70% reduction)
# - Faster guideline evaluation without affecting accuracy
# - Guidelines focus on recent context for applicability
#
# Recommended values:
# - Simple scenarios: 15 events (~7 rounds)
# - Standard scenarios (default): 20 events (~10 rounds)
# - Complex multi-guideline scenarios: 25-30 events (~12-15 rounds)
# - No limit: Set to 0 or negative (not recommended)
#
# Note: This is the MOST IMPORTANT optimization for token reduction
MAX_HISTORY_FOR_GUIDELINE_MATCHING=20

# Callback URL for async chat completion (optional)
# When a message is processed asynchronously via /chat_async, the result will be 
# posted to this URL with the correlation_id and chat response data
# Example: https://your-api.com/webhooks/chat-callback
CHAT_CALLBACK_HOST=http://callback-dev.ycloud.com